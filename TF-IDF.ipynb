{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here compute the TF-IDF on a corpus of newspaper headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:23:20.732834Z",
     "start_time": "2024-06-24T04:23:20.717141Z"
    }
   },
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import PorterStemmer"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into the file *headlines.csv*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:23:21.458385Z",
     "start_time": "2024-06-24T04:23:21.443551Z"
    }
   },
   "source": [
    "# TODO: Load the dataset\n",
    "headlines_data = pd.read_csv('headlines.csv')\n",
    "print(headlines_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      publish_date                                      headline_text\n",
      "0         20170721  algorithms can make decisions on behalf of fed...\n",
      "1         20170721  andrew forrests fmg to appeal pilbara native t...\n",
      "2         20170721                           a rural mural in thallan\n",
      "3         20170721  australia church risks becoming haven for abusers\n",
      "4         20170721  australian company usgfx embroiled in shanghai...\n",
      "...            ...                                                ...\n",
      "1994      20170624  constitution avenue wins top prize at act arch...\n",
      "1995      20170624                         dark mofo numbers crunched\n",
      "1996      20170624  david petraeus says australia must be firm on ...\n",
      "1997      20170624  driverless cars in australia face challenge of...\n",
      "1998      20170624           drug company criticised over price hikes\n",
      "\n",
      "[1999 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T04:23:22.145965Z",
     "start_time": "2024-06-24T04:23:22.131964Z"
    }
   },
   "source": [
    "# TODO: Have a look at the data\n",
    "headlines_data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20170721  algorithms can make decisions on behalf of fed...\n",
       "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
       "2      20170721                           a rural mural in thallan\n",
       "3      20170721  australia church risks becoming haven for abusers\n",
       "4      20170721  australian company usgfx embroiled in shanghai..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170721</td>\n",
       "      <td>algorithms can make decisions on behalf of fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170721</td>\n",
       "      <td>andrew forrests fmg to appeal pilbara native t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170721</td>\n",
       "      <td>a rural mural in thallan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australia church risks becoming haven for abusers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australian company usgfx embroiled in shanghai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T04:23:23.489932Z",
     "start_time": "2024-06-24T04:23:23.135484Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize Porter stemmer for stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_document(doc):\n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # Remove punctuation and convert to lower case\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stem the tokens\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the 'headline_text' column\n",
    "headlines_data['processed_text'] = headlines_data['headline_text'].apply(lambda x: preprocess_document(x))\n",
    "processed_headline = headlines_data['processed_text']\n",
    "# Print processed texts in the new dataframe\n",
    "print(processed_headline)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [algorithm, make, decis, behalf, feder, minist]\n",
      "1       [andrew, forrest, fmg, appeal, pilbara, nativ,...\n",
      "2                                 [rural, mural, thallan]\n",
      "3                  [australia, church, risk, becom, abus]\n",
      "4       [australian, compani, usgfx, embroil, shanghai...\n",
      "                              ...                        \n",
      "1994    [constitut, avenu, win, top, prize, act, archi...\n",
      "1995                         [dark, mofo, number, crunch]\n",
      "1996    [david, petraeu, say, australia, must, firm, s...\n",
      "1997    [driverless, car, australia, face, challeng, r...\n",
      "1998               [drug, compani, criticis, price, hike]\n",
      "Name: processed_text, Length: 1999, dtype: object\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:29:11.752693Z",
     "start_time": "2024-06-24T04:29:11.708694Z"
    }
   },
   "source": [
    "# TODO: Compute the BOW of the preprocessed data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "text_documents = [' '.join(doc) for doc in headlines_data['processed_text']]\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(text_documents)\n",
    "print(bow_matrix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 171)\t1\n",
      "  (0, 2307)\t1\n",
      "  (0, 1075)\t1\n",
      "  (0, 436)\t1\n",
      "  (0, 1439)\t1\n",
      "  (0, 2443)\t1\n",
      "  (1, 211)\t1\n",
      "  (1, 1528)\t1\n",
      "  (1, 1501)\t1\n",
      "  (1, 240)\t1\n",
      "  (1, 2816)\t1\n",
      "  (1, 2547)\t1\n",
      "  (1, 3842)\t1\n",
      "  (1, 3263)\t1\n",
      "  (2, 3268)\t1\n",
      "  (2, 2518)\t1\n",
      "  (2, 3784)\t1\n",
      "  (3, 330)\t1\n",
      "  (3, 788)\t1\n",
      "  (3, 3209)\t1\n",
      "  (3, 427)\t1\n",
      "  (3, 92)\t1\n",
      "  (4, 332)\t1\n",
      "  (4, 872)\t1\n",
      "  (4, 4013)\t1\n",
      "  :\t:\n",
      "  (1995, 1044)\t1\n",
      "  (1995, 2628)\t1\n",
      "  (1995, 2465)\t1\n",
      "  (1995, 1000)\t1\n",
      "  (1996, 330)\t1\n",
      "  (1996, 775)\t1\n",
      "  (1996, 3310)\t1\n",
      "  (1996, 3524)\t1\n",
      "  (1996, 3337)\t1\n",
      "  (1996, 1476)\t1\n",
      "  (1996, 2528)\t1\n",
      "  (1996, 1057)\t1\n",
      "  (1996, 2793)\t1\n",
      "  (1997, 330)\t1\n",
      "  (1997, 2939)\t1\n",
      "  (1997, 1398)\t1\n",
      "  (1997, 743)\t1\n",
      "  (1997, 690)\t1\n",
      "  (1997, 1223)\t1\n",
      "  (1997, 3246)\t1\n",
      "  (1998, 872)\t1\n",
      "  (1998, 1229)\t1\n",
      "  (1998, 2921)\t1\n",
      "  (1998, 1813)\t1\n",
      "  (1998, 987)\t1\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:30:30.567645Z",
     "start_time": "2024-06-24T04:30:30.531572Z"
    }
   },
   "source": [
    "# TODO: Compute the TF using the BOW\n",
    "\n",
    "# Compute the TF matrix\n",
    "tf_matrix = np.zeros_like(bow_matrix, dtype=float)\n",
    "\n",
    "# Iterate over each document\n",
    "for i in range(bow_matrix.shape[0]):\n",
    "    # Total number of terms in the current document\n",
    "    total_terms = np.sum(bow_matrix[i])\n",
    "    \n",
    "    # Compute TF for each term in the document\n",
    "    for j in range(bow_matrix.shape[1]):\n",
    "        tf_matrix[i, j] = bow_matrix[i, j] / total_terms if total_terms != 0 else 0\n",
    "\n",
    "# Convert TF matrix to DataFrame for better visualization\n",
    "tf_df = pd.DataFrame(tf_matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the TF DataFrame\n",
    "print(\"Term Frequency (TF) DataFrame:\")\n",
    "print(tf_df)"
   ],
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# Compute TF for each term in the document\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(bow_matrix\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]):\n\u001B[1;32m---> 13\u001B[0m         \u001B[43mtf_matrix\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m bow_matrix[i, j] \u001B[38;5;241m/\u001B[39m total_terms \u001B[38;5;28;01mif\u001B[39;00m total_terms \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Convert TF matrix to DataFrame for better visualization\u001B[39;00m\n\u001B[0;32m     16\u001B[0m tf_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(tf_matrix, columns\u001B[38;5;241m=\u001B[39mvectorizer\u001B[38;5;241m.\u001B[39mget_feature_names_out())\n",
      "\u001B[1;31mIndexError\u001B[0m: too many indices for array: array is 0-dimensional, but 2 were indexed"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T04:30:49.383143Z",
     "start_time": "2024-06-24T04:30:49.293049Z"
    }
   },
   "source": [
    "# TODO: Compute the IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Initialize TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True, smooth_idf=True)\n",
    "\n",
    "# Compute IDF from BoW matrix\n",
    "idf_values = tfidf_transformer.fit_transform(bow_matrix.values)\n",
    "\n",
    "# Convert IDF matrix to DataFrame for better visualization (optional)\n",
    "idf_dataframe = pd.DataFrame(idf_values.toarray(), columns=bow_matrix.columns)\n",
    "\n",
    "# Print IDF DataFrame\n",
    "print(idf_dataframe)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "values not found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m tfidf_transformer \u001B[38;5;241m=\u001B[39m TfidfTransformer(use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Compute IDF from BoW matrix\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m idf_values \u001B[38;5;241m=\u001B[39m tfidf_transformer\u001B[38;5;241m.\u001B[39mfit_transform(\u001B[43mbow_matrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Convert IDF matrix to DataFrame for better visualization (optional)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m idf_dataframe \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(idf_values\u001B[38;5;241m.\u001B[39mtoarray(), columns\u001B[38;5;241m=\u001B[39mbow_matrix\u001B[38;5;241m.\u001B[39mcolumns)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\datamining-lab7\\lib\\site-packages\\scipy\\sparse\\_base.py:771\u001B[0m, in \u001B[0;36mspmatrix.__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetnnz()\n\u001B[0;32m    770\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(attr \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m not found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: values not found"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:31:17.388383Z",
     "start_time": "2024-06-24T04:31:17.328379Z"
    }
   },
   "source": [
    "# TODO: compute the TF-IDF\n",
    "# Compute TF-IDF\n",
    "tfidf_matrix = bow_matrix.values * idf_dataframe.values\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame for better visualization (optional)\n",
    "tfidf_dataframe = pd.DataFrame(tfidf_matrix, columns=bow_matrix.columns)\n",
    "\n",
    "# Print TF-IDF DataFrame\n",
    "print(tfidf_dataframe)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "values not found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# TODO: compute the TF-IDF\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Compute TF-IDF\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m tfidf_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mbow_matrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m \u001B[38;5;241m*\u001B[39m idf_dataframe\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Convert TF-IDF matrix to DataFrame for better visualization (optional)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m tfidf_dataframe \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(tfidf_matrix, columns\u001B[38;5;241m=\u001B[39mbow_matrix\u001B[38;5;241m.\u001B[39mcolumns)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\datamining-lab7\\lib\\site-packages\\scipy\\sparse\\_base.py:771\u001B[0m, in \u001B[0;36mspmatrix.__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetnnz()\n\u001B[0;32m    770\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(attr \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m not found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: values not found"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T04:30:52.550041Z",
     "start_time": "2024-06-24T04:30:52.537004Z"
    }
   },
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "# Compute average TF-IDF across all documents\n",
    "avg_tfidf = np.mean(tfidf_dataframe, axis=0)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "avg_tfidf_df = pd.DataFrame({'word': tfidf_dataframe.columns, 'avg_tfidf': avg_tfidf})\n",
    "\n",
    "# Sort by average TF-IDF in descending order to get top 10 highest\n",
    "top_10_highest_tfidf = avg_tfidf_df.sort_values(by='avg_tfidf', ascending=False).head(10)\n",
    "\n",
    "# Sort by average TF-IDF in ascending order to get top 10 lowest\n",
    "top_10_lowest_tfidf = avg_tfidf_df.sort_values(by='avg_tfidf', ascending=True).head(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 words with highest average TF-IDF:\")\n",
    "print(top_10_highest_tfidf)\n",
    "\n",
    "print(\"\\nTop 10 words with lowest average TF-IDF:\")\n",
    "print(top_10_lowest_tfidf)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with highest average TF-IDF:\n",
      "  word  avg_tfidf\n",
      "         2.228225\n",
      "a    a   1.085073\n",
      "e    e   0.979820\n",
      "r    r   0.840612\n",
      "i    i   0.748304\n",
      "t    t   0.596771\n",
      "n    n   0.592459\n",
      "o    o   0.565829\n",
      "s    s   0.457424\n",
      "l    l   0.452727\n",
      "\n",
      "Top 10 words with lowest average TF-IDF:\n",
      "  word  avg_tfidf\n",
      "5    5   0.003974\n",
      "6    6   0.003982\n",
      "8    8   0.004155\n",
      "4    4   0.004432\n",
      "3    3   0.004779\n",
      "9    9   0.005889\n",
      "7    7   0.006320\n",
      "2    2   0.010632\n",
      "1    1   0.012802\n",
      "z    z   0.016472\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the TF-IDF using scikit learn\n",
    "# Import the module\n",
    "\n",
    "# Instantiate the TF-IDF vectorizer\n",
    "\n",
    "# Compute the TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest words: coll     0.305258\n",
      "gw       0.305258\n",
      "nmfc     0.305258\n",
      "adel     0.305258\n",
      "melb     0.305258\n",
      "syd      0.305258\n",
      "haw      0.305258\n",
      "geel     0.305258\n",
      "gcfc     0.305258\n",
      "fabio    0.322574\n",
      "dtype: float64\n",
      "highest words: mosul        0.779137\n",
      "rig          0.786813\n",
      "travel       0.788050\n",
      "aquapon      0.794899\n",
      "date         0.794899\n",
      "employ       0.795060\n",
      "financ       0.803629\n",
      "mongolian    0.831769\n",
      "pump         1.000000\n",
      "peacemak     1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
