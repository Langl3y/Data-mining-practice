{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04-Spam-Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to make our first real Machine Learning application of NLP: a spam classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A spam classifier is a Machine Learning model that classifier texts (email or SMS) into two categories: Spam (1) or legitimate (0).\n",
    "\n",
    "To do that, we will reuse our knowledge: we will apply preprocessing and BOW (Bag Of Words) on a dataset of texts.\n",
    "Then we will use a classifier to predict to which class belong a new email/SMS, based on the BOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first: import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T05:43:41.395079Z",
     "start_time": "2024-06-24T05:43:41.388081Z"
    }
   },
   "source": [
    "# Import NLTK and all the needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load now the dataset in *spam.csv* using pandas. Use the 'latin-1' encoding as loading option."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T05:43:42.204316Z",
     "start_time": "2024-06-24T05:43:42.185619Z"
    }
   },
   "source": [
    "# TODO: Load the dataset \n",
    "filename = \"spam.csv\"\n",
    "df = pd.read_csv(filename)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, I suggest you to explore a bit this dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T05:43:58.883562Z",
     "start_time": "2024-06-24T05:43:58.875563Z"
    }
   },
   "source": [
    "# TODO: explore the dataset\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class                                            Message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you see we have a column containing the labels, and a column containing the text to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by doing the usual preprocessing: tokenization, punctuation removal and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T05:44:43.749672Z",
     "start_time": "2024-06-24T05:44:41.488762Z"
    }
   },
   "source": [
    "# TODO: Perform preprocessing over all the text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation and stopwords\n",
    "    tokens = [t for t in tokens if t not in string.punctuation and t not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # Return cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'message' column\n",
    "df['clean_text'] = df['Message'].apply(preprocess_text)\n",
    "print(df['clean_text'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go jurong point crazy .. available bugis n gre...\n",
      "1                         ok lar ... joking wif u oni ...\n",
      "2       free entry 2 wkly comp win fa cup final tkts 2...\n",
      "3             u dun say early hor ... u c already say ...\n",
      "4                 nah n't think go usf life around though\n",
      "                              ...                        \n",
      "5567    2nd time tried 2 contact u. u �750 pound prize...\n",
      "5568                         �_ b going esplanade fr home\n",
      "5569                             pity mood ... suggestion\n",
      "5570    guy bitching acted like 'd interested buying s...\n",
      "5571                                       rofl true name\n",
      "Name: clean_text, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we have our preprocessed data. Next step is to do a BOW."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T05:45:56.006650Z",
     "start_time": "2024-06-24T05:45:55.896544Z"
    }
   },
   "source": [
    "# TODO: compute the BOW\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Convert BOW to DataFrame\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Concatenate BOW with the original DataFrame\n",
    "df_bow = pd.concat([df['Message'], bow_df], axis=1)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m X \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mfit_transform(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclean_text\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Convert BOW to DataFrame\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m bow_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(X\u001B[38;5;241m.\u001B[39mtoarray(), columns\u001B[38;5;241m=\u001B[39m\u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_feature_names\u001B[49m())\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Concatenate BOW with the original DataFrame\u001B[39;00m\n\u001B[0;32m      9\u001B[0m df_bow \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMessage\u001B[39m\u001B[38;5;124m'\u001B[39m], bow_df], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make a new dataframe as usual to have a visual idea of the words used and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T05:44:50.551323Z",
     "start_time": "2024-06-24T05:44:50.528326Z"
    }
   },
   "source": [
    "# TODO: Make a new dataframe with the BOW\n",
    "print(df_bow.head())"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# TODO: Make a new dataframe with the BOW\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdf_bow\u001B[49m\u001B[38;5;241m.\u001B[39mhead())\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_bow' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is the most used word in the spam category and the non spam category.\n",
    "\n",
    "There are two steps: first add the class to the BOW dataframe. Second, filter on a class, sum all the values and print the most frequent one."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T05:44:53.583640Z",
     "start_time": "2024-06-24T05:44:53.562639Z"
    }
   },
   "source": [
    "# TODO: print the most used word in the spam and non spam category\n",
    "print(\"Top words in spam messages:\")\n",
    "print(bow_df[df['label'] == 1].sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop words in non-spam messages:\")\n",
    "print(bow_df[df['label'] == 0].sum().sort_values(ascending=False).head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in spam messages:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bow_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# TODO: print the most used word in the spam and non spam category\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTop words in spam messages:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mbow_df\u001B[49m[df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39msort_values(ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTop words in non-spam messages:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(bow_df[df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39msort_values(ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'bow_df' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the most frequent spam word is 'free', not so surprising, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a classifier based on our BOW. We will use a simple logistic regression here for the example.\n",
    "\n",
    "You're an expert, you know what to do, right? Split the data, train your model, predict and see the performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Perform a classification to predict whether a message is a spam or not\n",
    "# Prepare data for classification\n",
    "X = bow_df.drop('label', axis=1)\n",
    "y = bow_df['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What precision do you get? Check by hand on some samples where it did predict well to check what could go wrong...\n",
    "\n",
    "Try to use other models and try to improve your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
